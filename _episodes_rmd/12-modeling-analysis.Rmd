---
title: "Creating univariate regression models"
teaching: 45
exercises: 10
questions:
- "How can I make a graph of my regression?"
- "How can I look at residuals?"
- "How can I assess and remove outliers?"
- "How can I export publication-quality graphics?"
objectives:
- "Make a scatterplot"
- "`cor()`"
- "Learn how to run regressions with `lm()` and `glm()`"
- "Learn how to isolate parts of the matrix that `lm()` returns"
- "Learn how to run `t.test()` - get p-value, CI, etc."
- "Assess interactions between variables"
keypoints:
- "FIXME"
source: Rmd
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("12-")
```

```{r, include=FALSE}
library(dplyr)

analysis_swan_df <- readRDS(file = "data_out/analysis_swan_df_08.rds")
colnames(analysis_swan_df)
```

# Regression models in R

Now that we have cleaned up the variables in our data frame, and are sufficiently satisfied that the variables of interest meet our assumptions for regression, we can proceed with building regression models.

Most of the commonly-used statistical tests and model-building functions are in R's built-in `stats` package, although there are certainly many other packages available with additional statistical functionality.

For simple linear regression, we can use the `lm()` function (`lm` stands for "linear model").  R also provides `glm()` for more generalized regression, which you can use not only for linear regression but also for logistic regression, Poisson regression, etc.  R's `stats()` package also has `anova()` and many other commonly used statistical computations.

We'll need to specify some parameters when we use `lm()`.  At a minumum, we need to specify:
* The data frame containing our data
* The formula for the model we'd like to fit.  In other words, which variable in our data frame is the "y" variable, and which combination of "x" variables are we including in our model.

There are other optional parameters available in `lm()` as well.

Our outcome variable of interest is `Glucose` which is a continuous variable, so linear regression is appropriate.  (If our outcome variable was a categorical variable, we'd be performing a logistic regression, for which we'd need to use `glm()`)

Before we build a full model, we will first assess the individual impact of each variable, independently, on the outcome, by conducting univariate regressions on each variable versus the outcome.

As we build these models, we'll keep an eye on the R-squared values and the F-test values.  We'll also decide on a cutoff for a *p*-value as a criteria for which variables to include in the multivariate model.  Let's use a *p*-value cutoff of 0.15.

For our first univariate model, we'll try fitting the following model:

$$
Glucose = \beta_0 + \beta_1bp\_category + \varepsilon
$$

Where $$\beta_0$$ is the y-intercept and $$\beta_1$$ is the slope, and $$\varepsilon$$ is the error term.

To compute this regression using `lm()`, we need to specify the `forumla` parameter and the `data` parameter.

The `formula` parameter expects an R formula object, which is expressed using a tilde (`~`) to relate a "y" variable to an expression containing "x" variables.  For example, formulas might look like:

`y ~ x`

`lung_cancer ~ smoker + age + gender + income`

Formulas can also include [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics)):

`y ~ x1 + x2 + x1*x2`

Note that the names of the variables must be variables that are present in the data frame specified in the `data` parameter.

In our case, we have a relatively simple univariate model where `Glucose` is the "y" variable, and `BMI_cat` is our only "x" variable:

`Glucose ~ BMI_cat`

We'll store the resule of the regression in a variable we'll call `lm_bmi`: 

```{r}
lm_bmi <- lm(Glucose ~ BMI_cat, data = analysis_swan_df)
summary(lm_bmi) # p<0.001 for all categories except underweight--> KEEP
```

Let's now take a look at the model that `lm()` computed:
#TODO functions that pull out different pieces of lm. 
Note that reference level is "normal" which is not shown as one of the categories of bp_category. 


```{r}
lm_age <- lm(Glucose ~ Age, data = analysis_swan_df)
```

```{r}
summary(lm_age) # p=0.231 --> We may not keep it then.
```

```{r}
lm_bp <- lm(formula = Glucose ~ bp_category, data = analysis_swan_df)
```

```{r}
summary(lm_bp) # p<0.001 --> KEEP
```

```{r}
lm_Chol_Ratio <- lm(Glucose ~ Chol_Ratio, data = analysis_swan_df)
```

```{r}
summary(lm_Chol_Ratio) # p<0.001 --> KEEP
```

```{r}
lm_race <- lm(Glucose ~ RACE, data = analysis_swan_df)
```

```{r}
summary(lm_race) # p=0.207 is the smallest --> can keep but can also drop. let's see what the stepwise does. 
```

```{r}
lm_exercise <- lm(Glucose ~ Exercise, data = analysis_swan_df)
```

```{r}
summary(lm_exercise) # p<0.001 --> KEEP
```

```{r}
lm_crp <- lm(Glucose ~ log_CRP, data = analysis_swan_df)
```

```{r}
summary(lm_crp) # p<0.001 --> KEEP
```

#TODO Fix smoker lm and do everything below
```{r}
lm_smoke <- lm(Glucose ~ Smoker, data = analysis_swan_df)
```

```{r}
summary(lm_smoke) # p=0.0351 --> KEEP B/C LESS THAN 0.05
```

Next, we need to consider "Interaction terms". See link at the top of this episode. 
```{r}
analysis_swan_df$age_Chol_Ratio <- analysis_swan_df$Chol_Ratio * analysis_swan_df$Age
```

```{r}
lm_ageChol <- lm(Glucose ~ age_Chol_Ratio, data = analysis_swan_df)
summary(lm_ageChol) # This is significant on it's own.
```

```{r, include=FALSE}
# Save as RData format
saveRDS(analysis_swan_df, file = "data_out/analysis_swan_df_12.rds")
```

